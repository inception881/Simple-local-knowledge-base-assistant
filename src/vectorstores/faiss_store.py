"""
FAISS å‘é‡å­˜å‚¨ç®¡ç†

è¯¥æ¨¡å—æä¾›äº†å¯¹ FAISS å‘é‡å­˜å‚¨çš„ç»Ÿä¸€ç®¡ç†æ¥å£ï¼ŒåŒ…æ‹¬åˆ›å»ºã€åŠ è½½ã€æ›´æ–°å’ŒæŸ¥è¯¢ç­‰åŠŸèƒ½ã€‚
FAISS æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å‘é‡ç›¸ä¼¼åº¦æœç´¢åº“ï¼Œç”¨äºå­˜å‚¨æ–‡æ¡£çš„å‘é‡è¡¨ç¤ºå¹¶è¿›è¡Œå¿«é€Ÿæ£€ç´¢ã€‚
"""
import uuid
from pathlib import Path
from typing import List, Optional, Dict, Any, Union
import hashlib

from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.vectorstores import VectorStore

from src.config import Config
from src.embedding import get_embeddings, get_embeddings_singleton
from src.loaders.document_loader import get_document_loader

# å‘é‡åº“å­˜å‚¨è·¯å¾„
FAISS_INDEX_PATH = Config.FAISS_INDEX_PATH

# ç¡®ä¿ç›®å½•å­˜åœ¨
FAISS_INDEX_PATH.mkdir(parents=True, exist_ok=True)

class FAISSVectorStore:
    """FAISS å‘é‡å­˜å‚¨ç®¡ç†ç±»"""
    
    def __init__(self, embeddings: Optional[Embeddings] = None):
        """
        åˆå§‹åŒ– FAISS å‘é‡å­˜å‚¨ç®¡ç†å™¨
        
        Args:
            embeddings: åµŒå…¥æ¨¡å‹ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€å•ä¾‹
        """
        self.embeddings = embeddings or get_embeddings_singleton()
        self.vector_store = self._load_or_create_vector_store()
    
    def _load_or_create_vector_store(self) -> FAISS:
        """
        åŠ è½½æˆ–åˆ›å»º FAISS å‘é‡å­˜å‚¨
        
        å¦‚æœå­˜åœ¨å·²ä¿å­˜çš„å‘é‡åº“ï¼Œåˆ™åŠ è½½ï¼›å¦åˆ™åˆ›å»ºä¸€ä¸ªç©ºçš„å‘é‡åº“ã€‚
        
        Returns:
            FAISS å‘é‡å­˜å‚¨å®ä¾‹
        """
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²ä¿å­˜çš„å‘é‡åº“
        if FAISS_INDEX_PATH.exists() and any(FAISS_INDEX_PATH.iterdir()):
            try:
                print(f"âœ… åŠ è½½å·²æœ‰å‘é‡åº“ï¼ˆè·¯å¾„ï¼š{FAISS_INDEX_PATH}ï¼‰")
                vector_store = FAISS.load_local(
                    folder_path=str(FAISS_INDEX_PATH),
                    embeddings=self.embeddings,
                    allow_dangerous_deserialization=True
                )
                return vector_store
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å‘é‡åº“å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°çš„å‘é‡åº“")
        
        # å¦‚æœæ²¡æœ‰æ–‡æ¡£ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„å‘é‡å­˜å‚¨
        print("âš ï¸ æœªæ‰¾åˆ°å·²æœ‰å‘é‡åº“æˆ–åŠ è½½å¤±è´¥ï¼Œåˆ›å»ºç©ºçš„å‘é‡å­˜å‚¨")
        vector_store = FAISS.from_texts(
            ["åˆå§‹åŒ–æ–‡æ¡£"], self.embeddings
        )
        vector_store.save_local(str(FAISS_INDEX_PATH))
        return vector_store
    
    def get_retriever(self, k: int = None):
        """
        è·å–æ£€ç´¢å™¨
        
        Args:
            k: æ£€ç´¢çš„æ–‡æ¡£æ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            
        Returns:
            æ£€ç´¢å™¨å®ä¾‹
        """
        search_kwargs = {"k": k or Config.TOP_K}
        return self.vector_store.as_retriever(search_kwargs=search_kwargs)
    
    def add_documents(self, documents: List[Document], batch_size: int = 10, ids: Optional[List[str]] = None) -> bool:
        """
        æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
        
        Args:
            documents: è¦æ·»åŠ çš„æ–‡æ¡£chunkåˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            ids: å¯é€‰çš„æ–‡æ¡£IDåˆ—è¡¨ï¼Œå¦‚æœæä¾›ï¼Œé•¿åº¦å¿…é¡»ä¸documentsç›¸åŒ
            
        Returns:
            æ˜¯å¦æˆåŠŸæ·»åŠ æ–‡æ¡£
        """
        if not documents:
            print("âš ï¸ æ— æ–‡æ¡£å¯æ·»åŠ ")
            return False
        
        # ç”Ÿæˆæ–‡æ¡£ID
        if ids is None:
            # ä¸ºæ¯ä¸ªæ–‡æ¡£ç”Ÿæˆä¸€ä¸ªIDï¼Œæ ¼å¼ä¸ºï¼šæºæ–‡ä»¶è·¯å¾„_UUID
            generated_ids = []
            for doc in documents:
                source = doc.metadata.get("source", "")
                doc_uuid = str(uuid.uuid4())
                generated_ids.append(f"{source}_{doc_uuid}")
            ids = generated_ids
        
        # ç¡®ä¿æ–‡æ¡£å’ŒIDæ•°é‡ä¸€è‡´
        if len(documents) != len(ids):
            raise ValueError(f"æ–‡æ¡£æ•°é‡({len(documents)})ä¸IDæ•°é‡({len(ids)})ä¸åŒ¹é…")
        
        # ä½¿ç”¨æ–‡æ¡£åŠ è½½å™¨æœåŠ¡çš„æ‰¹å¤„ç†åŠŸèƒ½
        loader = get_document_loader()
        batches = loader.batch_process_documents(documents, batch_size)
        id_batches = [ids[i:i+batch_size] for i in range(0, len(ids), batch_size)]
        print(f"âœ… æ–‡æ¡£åˆ†æ‰¹å®Œæˆï¼šå…± {len(batches)} ä¸ªæ‰¹æ¬¡")
        
        # å‘é‡åŒ–å¹¶åˆå¹¶åˆ°å‘é‡åº“ï¼ˆä½¿ç”¨é¢„å…ˆåˆ†å¥½çš„æ‰¹æ¬¡ï¼‰
        for i, (batch, batch_ids) in enumerate(zip(batches, id_batches)):
            if self.vector_store is None:
                self.vector_store = FAISS.from_documents(batch, self.embeddings, ids=batch_ids)
            else:
                # ä½¿ç”¨add_documentsè€Œä¸æ˜¯from_documentså’Œmerge_fromï¼Œä»¥ä¾¿ä¼ é€’IDs
                self.vector_store.add_documents(documents=batch, ids=batch_ids)
            print(f"âœ… å·²å¤„ç†æ‰¹æ¬¡ {i+1}/{len(batches)}")
        
        # ä¿å­˜æ›´æ–°åçš„å‘é‡åº“åˆ°æœ¬åœ°
        if self.vector_store:
            self.vector_store.save_local(str(FAISS_INDEX_PATH))
            print(f"âœ… å‘é‡åº“æ›´æ–°å®Œæˆï¼Œå·²ä¿å­˜åˆ°ï¼š{FAISS_INDEX_PATH}")
            return True
        
        return False
    

    
    def search(self, query: str, k: int = None) -> List[Document]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›çš„æ–‡æ¡£æ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            
        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        return self.vector_store.similarity_search(query, k=k or Config.TOP_K)
    
    def search_with_score(self, query: str, k: int = None) -> List[tuple]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£å¹¶è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›çš„æ–‡æ¡£æ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼
            
        Returns:
            (æ–‡æ¡£, åˆ†æ•°) å…ƒç»„åˆ—è¡¨
        """
        return self.vector_store.similarity_search_with_score(query, k=k or Config.TOP_K)
    
    def save(self, path: Optional[str] = None) -> None:
        """
        ä¿å­˜å‘é‡åº“åˆ°æœ¬åœ°
        
        Args:
            path: ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
        """
        save_path = path or str(FAISS_INDEX_PATH)
        self.vector_store.save_local(save_path)
        print(f"âœ… å‘é‡åº“å·²ä¿å­˜åˆ°ï¼š{save_path}")
    
    # def load_documents_and_update(self, document_paths: List[str]) -> bool:
    #     """
    #     åŠ è½½æ–‡æ¡£å¹¶æ›´æ–°å‘é‡åº“
        
    #     Args:
    #         document_paths: æ–‡æ¡£è·¯å¾„åˆ—è¡¨
            
    #     Returns:
    #         æ˜¯å¦æˆåŠŸæ›´æ–°
    #     """
    #     print(f"\nğŸ“š å¼€å§‹æ›´æ–°çŸ¥è¯†åº“ï¼ˆæ–°æ–‡æ¡£æ•°ï¼š{len(document_paths)}ï¼‰")
        
    #     # ä½¿ç”¨æ–‡æ¡£åŠ è½½å™¨æœåŠ¡åŠ è½½æ–‡æ¡£
    #     loader = get_document_loader()
    #     all_docs = loader.process_documents(document_paths, skip_processed=True)
        
    #     if not all_docs:
    #         print("âš ï¸ æ— æ–°å¢æ–‡æ¡£ï¼ŒçŸ¥è¯†åº“æœªæ›´æ–°")
    #         return False
        
    #     print(f"âœ… æˆåŠŸåŠ è½½ {len(all_docs)} ä¸ªæ–°æ–‡æ¡£")
        
    #     # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡åº“
    #     return self.add_documents(all_docs)
    
    def delete(self, ids: List[str]) -> bool:
        """
        ä»å‘é‡åº“ä¸­åˆ é™¤æŒ‡å®šIDçš„æ–‡æ¡£
        
        Args:
            ids: è¦åˆ é™¤çš„æ–‡æ¡£IDåˆ—è¡¨
            
        Returns:
            æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        if not ids:
            print("âš ï¸ æœªæä¾›è¦åˆ é™¤çš„æ–‡æ¡£ID")
            return False
        
        try:
            self.vector_store.delete(ids=ids)
            # ä¿å­˜æ›´æ–°åçš„å‘é‡åº“åˆ°æœ¬åœ°
            self.save()
            print(f"âœ… æˆåŠŸåˆ é™¤ {len(ids)} ä¸ªæ–‡æ¡£")
            return True
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def delete_by_source(self, source_paths: List[str]) -> bool:
        """
        æ ¹æ®æºæ–‡ä»¶è·¯å¾„åˆ é™¤æ–‡æ¡£
        
        åˆ é™¤æ‰€æœ‰IDä»¥æŒ‡å®šæºæ–‡ä»¶è·¯å¾„å¼€å¤´çš„æ–‡æ¡£ï¼ˆå¿½ç•¥UUIDéƒ¨åˆ†ï¼‰
        
        Args:
            source_paths: æºæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        if not source_paths:
            print("âš ï¸ æœªæä¾›è¦åˆ é™¤çš„æºæ–‡ä»¶è·¯å¾„")
            return False
        
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£ID
            all_ids = list(self.vector_store.index_to_docstore_id.values())
            
            # æ‰¾å‡ºåŒ¹é…çš„ID
            ids_to_delete = []
            for source_path in source_paths:
                for doc_id in all_ids:
                    # æ£€æŸ¥IDæ˜¯å¦ä»¥æºæ–‡ä»¶è·¯å¾„å¼€å¤´ï¼ˆæ ¼å¼ä¸ºsource_path_uuidï¼‰
                    if doc_id.startswith(f"{source_path}_"):
                        ids_to_delete.append(doc_id)
            
            if not ids_to_delete:
                print("âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡æ¡£")
                return False
            
            # åˆ é™¤åŒ¹é…çš„æ–‡æ¡£
            self.vector_store.delete(ids=ids_to_delete)
            
            # ä¿å­˜æ›´æ–°åçš„å‘é‡åº“åˆ°æœ¬åœ°
            self.save()
            print(f"âœ… æˆåŠŸåˆ é™¤ {len(ids_to_delete)} ä¸ªæ–‡æ¡£")
            return True
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def clear(self) -> None:
        """æ¸…ç©ºå‘é‡åº“"""
        # åˆ›å»ºä¸€ä¸ªæ–°çš„ç©ºå‘é‡åº“
        self.vector_store = FAISS.from_texts(
            ["åˆå§‹åŒ–æ–‡æ¡£"], self.embeddings
        )
        self.save()
        print("âœ… å‘é‡åº“å·²æ¸…ç©º")


# å…¨å±€å•ä¾‹
_vector_store_instance = None

def get_faiss_vector_store() -> FAISSVectorStore:
    """
    è·å– FAISS å‘é‡å­˜å‚¨å•ä¾‹
    
    Returns:
        FAISSVectorStore å•ä¾‹å®ä¾‹
    """
    global _vector_store_instance
    if _vector_store_instance is None:
        _vector_store_instance = FAISSVectorStore()
    return _vector_store_instance
